{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CLIP-I, CLIP-T, DINO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **CLIP-I** - average pairwise cosine similarity between CLIP embeddings of generated and real images\n",
    "- **CLIP-T** - average cosine similarity between prompt and CLIP embeddings of generated images\n",
    "- **DINO** - average pairwise cosine similarity between the Vit-S/16 DINO embeddings of generated and real images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoModel, AutoProcessor, AutoTokenizer\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from diffusers import DiffusionPipeline, StableDiffusionXLImg2ImgPipeline\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_outs(pretrained_model_name_or_path: str, prompts:list, samples_per_prompt: int):\n",
    "    model_path = pretrained_model_name_or_path\n",
    "    diffusion_pipe = DiffusionPipeline.from_pretrained(\n",
    "        model_path, torch_dtype=torch.float16\n",
    "    )\n",
    "    diffusion_pipe = diffusion_pipe.to(\"cuda\")\n",
    "    generator = torch.Generator(\"cuda\")\n",
    "    generator = generator.manual_seed(0)\n",
    "\n",
    "    tasks_prompts = []\n",
    "    tasks_samples = []\n",
    "    for current_prompt in prompts:\n",
    "        task_prompts = [current_prompt] * samples_per_prompt\n",
    "        task_samples = diffusion_pipe(\n",
    "            prompt=task_prompts, output_type=\"pil\", generator=generator\n",
    "        )\n",
    "        task_prompts.append(task_prompts)\n",
    "        tasks_samples.append(task_samples)\n",
    "\n",
    "    return tasks_prompts, list(itertools.chain.from_iterable([out.images for out in tasks_samples]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TASKS = 5\n",
    "MODELS = [\"wolf_plushie_sd1\", \"backpack_sd2\", \"dog6_sd3\", \"candle_sd4\", \"cat2_sd5\"]\n",
    "TOKENS = [\"sks stuffed animal\", \"zwz backpack\", \"sbu dog\", \"uwu candle\", \"pdw cat\"]\n",
    "AFTER_TASK_MODEL = {\n",
    "    k+1:v for k,v in zip(range(N_TASKS), MODELS)\n",
    "}\n",
    "TOKEN_TASK = {\n",
    "    k+1: v for k,v in zip(range(N_TASKS), MODELS)\n",
    "}\n",
    "\n",
    "PROMPTS = ['a {} in a purple wizard outfit', 'a {} in a police outfit', 'a {} wearing a santa hat', 'a {} in a jail', 'a {} looking into a mirror']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(task_number):\n",
    "    assert task_number >= 1\n",
    "    per_task_outs = {}\n",
    "    for curr_task_number in range(task_number):\n",
    "        task_outs = {}\n",
    "        model_path = f\"./models/{AFTER_TASK_MODEL[curr_task_number+1]}\"\n",
    "        prompts = [\n",
    "            prompt.format(TOKEN_TASK[curr_task_number+1]) for prompt in PROMPTS\n",
    "        ]\n",
    "        out_prompts, out_samples = get_model_outs(pretrained_model_name_or_path=model_path, prompts=prompts, samples_per_prompt=8)\n",
    "        task_outs[\"prompts\"] = out_prompts\n",
    "        task_outs[\"samples\"] = out_samples\n",
    "        per_task_outs[curr_task_number+1] = task_outs\n",
    "    return per_task_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880d5301315f417d97b526ec822d4a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b84082d4354b2c8ce58e1abc79a2d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/tscratch/people/plglukaszst/envs/lora/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc4530e79f3845a8b7618188c3c863b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "870876e177ce4eb7a3a7f8e4293bf864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af9833d068c4f6ab457102a5a445990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f117108f91542e5addca92b167995c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_outs_1 = run_model(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41cf9b5419b8409b9121afddf431361c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ff1a68d07f435281c5294ee11b9526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_outs_2 = run_model(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_outs_3 = run_model(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_outs_4 = run_model(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_outs_5 = run_model(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = image_grid(imgs, rows=8, cols=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path_1 = '/net/tscratch/people/plgkzaleska/ziplora-analysis/data/dreambooth/dataset/rc_car/01.jpg'\n",
    "img_path_2 = '/net/tscratch/people/plgkzaleska/ziplora-analysis/data/dreambooth/dataset/rc_car/02.jpg'\n",
    "img_path_3 = '/net/tscratch/people/plgkzaleska/ziplora-analysis/data/dreambooth/dataset/rc_car/03.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = Image.open(img_path_1)\n",
    "image2 = Image.open(img_path_2)\n",
    "image3 = Image.open(img_path_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display images in figure\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 15))\n",
    "ax[0].imshow(image1)\n",
    "ax[1].imshow(image2)\n",
    "ax[2].imshow(image3)\n",
    "# hide the axes\n",
    "for ax in ax:\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **CLIP-I**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model = AutoModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs1 = clip_processor(images=image1, return_tensors=\"pt\").to(device)\n",
    "image_features1 = clip_model.get_image_features(**inputs1)\n",
    "\n",
    "inputs2 = clip_processor(images=image2, return_tensors=\"pt\").to(device)\n",
    "image_features2 = clip_model.get_image_features(**inputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = F.cosine_similarity(image_features1[0], image_features2[0], dim=0)\n",
    "print(f\"Similarity: {sim.item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **CLIP-T**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"a photo of a toy car\"\n",
    "text = clip_tokenizer(input_text, return_tensors=\"pt\", padding=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = clip_processor(images=image1, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embeddings = clip_model.get_image_features(**input_img)\n",
    "text_features = clip_model.get_text_features(**text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = F.cosine_similarity(img_embeddings[0], text_features[0], dim=0)\n",
    "print(f\"Similarity: {sim.item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **DINO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dino_model = AutoModel.from_pretrained(\"facebook/dino-vits16\", add_pooling_layer=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=3),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get DINO features\n",
    "inputs1 = T(image1).unsqueeze(0).to(device)\n",
    "image_features1 = dino_model(inputs1).last_hidden_state\n",
    "\n",
    "inputs2 = T(image2).unsqueeze(0).to(device)\n",
    "image_features2 = dino_model(inputs2).last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = F.cosine_similarity(image_features1[0, 0], image_features2[0, 0], dim=0)\n",
    "print('Similarity:', sim.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
