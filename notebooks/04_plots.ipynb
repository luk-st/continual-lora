{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval.eval_method import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_results = Path(\"results/object/base/after_task_10\").resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_base_object = [\n",
    "    load_pickle(path_results / Path(f\"on_task_{task_idx}.pkl\"))[\"samples\"] for task_idx in range(1, 11)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_results = Path(\"results/object/base/after_task_10\").resolve()\n",
    "imgs_base_object = [\n",
    "    load_pickle(path_results / Path(f\"on_task_{task_idx}.pkl\"))[\"samples\"] for task_idx in range(1, 11)\n",
    "]\n",
    "path_config = Path(\"data/data_object/config.json\").resolve()\n",
    "with open(path_config) as f:\n",
    "    file = json.load(f)\n",
    "    tasks = file[\"tasks\"]\n",
    "\n",
    "    tasks_map = {int(idx) + 1: task for idx, task in enumerate(tasks)}\n",
    "    for task_k, task_v in tasks_map.items():\n",
    "        task_v[\"index\"] = task_k\n",
    "\n",
    "results = {}\n",
    "for task_idx in [task_idx for task_idx in tasks_map.keys()]:\n",
    "    on_task_samples = imgs_base_object[task_idx - 1]\n",
    "    eval_path = get_task_eval_path(tasks_map[task_idx], \"object\")\n",
    "    results[task_idx] = calculate_metrics(\n",
    "        model_samples=on_task_samples, eval_path=eval_path, task_type=\"object\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_res_from_path(path):\n",
    "    path = Path(path).resolve()\n",
    "    with open(path) as f:\n",
    "        tasks_config = json.load(f)\n",
    "\n",
    "    new_tasks_map = []\n",
    "    for task in tasks_config['tasks']:\n",
    "        for idx, task_base in tasks_map.items():\n",
    "            if task['prompt'] == task_base['prompt']:\n",
    "                new_tasks_map.append(idx)\n",
    "                break\n",
    "    return new_tasks_map\n",
    "\n",
    "task_map_1 = load_res_from_path(\"models/naive_cl/seed_0_object/seed_0_order/config.json\")\n",
    "task_map_2 = load_res_from_path(\"models/naive_cl/seed_0_object/seed_5_order/config.json\")\n",
    "task_map_3 = load_res_from_path(\"models/naive_cl/seed_0_object/seed_10_order/config.json\")\n",
    "task_map_4 = load_res_from_path(\"models/naive_cl/seed_0_object/seed_42_order/config.json\")\n",
    "\n",
    "results1 = {\n",
    "    task_idx: results[task_map_1[task_idx]] for task_idx in range(len(task_map_1))\n",
    "}\n",
    "results2 = {\n",
    "    task_idx: results[task_map_2[task_idx]] for task_idx in range(len(task_map_2))\n",
    "}\n",
    "results3 = {\n",
    "    task_idx: results[task_map_3[task_idx]] for task_idx in range(len(task_map_3))\n",
    "}\n",
    "results4 = {\n",
    "    task_idx: results[task_map_4[task_idx]] for task_idx in range(len(task_map_4))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PATH_METRICS_ROOT = Path(\"./results/metrics/\")\n",
    "OBJECT_METRICS = {\n",
    "    \"clip\": clip_image_metric,\n",
    "    \"dino\": dino_metric,\n",
    "}\n",
    "\n",
    "results_final = {\n",
    "    metric_name: {\n",
    "        task_idx + 1: np.mean([results1[task_idx][metric_name], results2[task_idx][metric_name], results3[task_idx][metric_name], results4[task_idx][metric_name]])\n",
    "        for task_idx in range(10)\n",
    "    }\n",
    "    for metric_name in OBJECT_METRICS.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_config = Path(\"data/data_object/config.json\").resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_config) as f:\n",
    "    file = json.load(f)\n",
    "    tasks = file[\"tasks\"]\n",
    "\n",
    "    tasks_map = {int(idx) + 1: task for idx, task in enumerate(tasks)}\n",
    "    for task_k, task_v in tasks_map.items():\n",
    "        task_v[\"index\"] = task_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PATH_METRICS_ROOT = Path(\"./results/metrics/\")\n",
    "OBJECT_METRICS = {\n",
    "    \"clip\": clip_image_metric,\n",
    "    \"dino\": dino_metric,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for task_idx in [task_idx for task_idx in tasks_map.keys()]:\n",
    "    on_task_samples = imgs_base_object[task_idx - 1]\n",
    "    eval_path = get_task_eval_path(tasks_map[task_idx], \"object\")\n",
    "    results[task_idx] = calculate_metrics(\n",
    "        model_samples=on_task_samples, eval_path=eval_path, task_type=\"object\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = [metrics_results[\"clip\"] for metrics_results in results.values()]\n",
    "dino = [metrics_results[\"dino\"] for metrics_results in results.values()]\n",
    "np.mean(clip, axis=0), np.mean(dino, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "METRIC_NAME = \"dino\"\n",
    "METHOD_NAME = \"mag_max_light\"\n",
    "METHOD_NAME2 = \"naive_cl\"\n",
    "METHOD_NAME3 = \"ortho_init\"\n",
    "METHOD_NAME4 = \"merge_and_init\"\n",
    "TASK_IDX = 1\n",
    "\n",
    "METHOD_TO_NAME = {\n",
    "    \"mag_max_light\": \"Magnitude-based Merging\",\n",
    "    \"naive_cl\": \"Na誰ve Continual Fine-Tuning\",\n",
    "    \"ortho_init\": \"Merge & Orthogonal Initialization\",\n",
    "    \"merge_and_init\": \"Merge & Initialization\",\n",
    "}\n",
    "\n",
    "METRIC_TO_NAME = {\n",
    "    \"clip\": \"CLIP-I\",\n",
    "    \"dino\": \"DINO\",\n",
    "}\n",
    "\n",
    "path = Path(f\"results/metrics/object/{METHOD_NAME}/metrics_avg.pkl\").resolve()\n",
    "path2 = Path(f\"results/metrics/object/{METHOD_NAME2}/metrics_avg.pkl\").resolve()\n",
    "path3 = Path(f\"results/metrics/object/{METHOD_NAME3}/metrics_avg.pkl\").resolve()\n",
    "path4 = Path(f\"results/metrics/object/{METHOD_NAME4}/metrics_avg.pkl\").resolve()\n",
    "\n",
    "with open(path, \"rb\") as f:\n",
    "    metrics = pickle.load(f)[METRIC_NAME]\n",
    "\n",
    "with open(path2, \"rb\") as f:\n",
    "    metrics2 = pickle.load(f)[METRIC_NAME]\n",
    "\n",
    "with open(path3, \"rb\") as f:\n",
    "    metrics3 = pickle.load(f)[METRIC_NAME]\n",
    "\n",
    "with open(path4, \"rb\") as f:\n",
    "    metrics4 = pickle.load(f)[METRIC_NAME]\n",
    "\n",
    "# data1 = metrics[('5', '0')]\n",
    "# data2 = metrics[('5', '5')]\n",
    "\n",
    "# data12 = metrics2[('5', '0')]\n",
    "# data22 = metrics2[('5', '5')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "exclude_models = []\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"axes.facecolor\": \"#f0f0f0\",\n",
    "        \"figure.facecolor\": \"#ffffff\",\n",
    "        \"grid.color\": \"white\",\n",
    "        \"axes.edgecolor\": \"#393939\",\n",
    "        \"axes.linewidth\": 1.0,\n",
    "        \"axes.labelcolor\": \"#393939\",\n",
    "        \"xtick.color\": \"#393939\",\n",
    "        \"ytick.color\": \"#393939\",\n",
    "        \"font.size\": 12,\n",
    "        \"legend.fontsize\": 10,\n",
    "        \"legend.title_fontsize\": 10,\n",
    "        \"figure.figsize\": (14, 7),\n",
    "    }\n",
    ")\n",
    "\n",
    "tasks = defaultdict()\n",
    "tasks2 = defaultdict()\n",
    "tasks3 = defaultdict()\n",
    "tasks4 = defaultdict()\n",
    "\n",
    "data = metrics\n",
    "data2 = metrics2\n",
    "data3 = metrics3\n",
    "data4 = metrics4\n",
    "data3 = metrics3\n",
    "\n",
    "\n",
    "labels = [\"\" for _ in range(len(list(data.keys())) + 1)]\n",
    "for (model_idx, task_idx) in data.keys():\n",
    "    if task_idx != TASK_IDX:\n",
    "        continue\n",
    "    labels[model_idx] = f\"#{model_idx} tasks\"\n",
    "    tasks[model_idx] = data[(model_idx, TASK_IDX)]\n",
    "\n",
    "for (model_idx, task_idx) in data2.keys():\n",
    "    if task_idx != TASK_IDX:\n",
    "        continue\n",
    "    tasks2[model_idx] = data2[(model_idx, TASK_IDX)]\n",
    "\n",
    "for (model_idx, task_idx) in data3.keys():\n",
    "    if task_idx != TASK_IDX:\n",
    "        continue\n",
    "    tasks3[model_idx] = data3[(model_idx, TASK_IDX)]\n",
    "\n",
    "for (model_idx, task_idx) in data4.keys():\n",
    "    if task_idx != TASK_IDX:\n",
    "        continue\n",
    "    tasks4[model_idx] = data4[(model_idx, TASK_IDX)]\n",
    "\n",
    "\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, 11))\n",
    "tasks[0] = {}\n",
    "labels[0] = \"Base model\"\n",
    "tasks[0] = results_final[METRIC_NAME][TASK_IDX]\n",
    "\n",
    "# Start a new figure (ensure it's created before plotting)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot each model's performance over the tasks\n",
    "x = [model_idx for model_idx in tasks.keys() if model_idx not in [0] + exclude_models]\n",
    "y = [tasks[model_idx] for model_idx in x]\n",
    "y[0] = 0.44837309792637825\n",
    "ax.plot(x, y, label=METHOD_TO_NAME[METHOD_NAME], color=\"rebeccapurple\", linewidth=3, markersize=7, linestyle=\"-\")\n",
    "\n",
    "y2 = [tasks2[model_idx] for model_idx in x]\n",
    "y2[0] = 0.44837309792637825\n",
    "ax.plot(x, y2, label=METHOD_TO_NAME[METHOD_NAME2], color=\"darkcyan\", linewidth=3, markersize=7, linestyle=\"-\")\n",
    "\n",
    "y3 = [tasks3[model_idx] for model_idx in x]\n",
    "y3[0] = 0.44837309792637825\n",
    "ax.plot(x, y3, label=METHOD_TO_NAME[METHOD_NAME3], color=\"goldenrod\", linewidth=3, markersize=7, linestyle=\"-\")\n",
    "\n",
    "y4 = [tasks4[model_idx] for model_idx in x]\n",
    "y4[0] = 0.44837309792637825\n",
    "ax.plot(x, y4, label=METHOD_TO_NAME[METHOD_NAME4], color=\"indianred\", linewidth=3, markersize=7, linestyle=\"-\")\n",
    "\n",
    "\n",
    "plt.axhline(y=tasks[0], color='black', linestyle='--', linewidth=3, label=\"Base model performance\")\n",
    "\n",
    "# for idx, (model, task_data) in enumerate(tasks.items()):\n",
    "#     if model in exclude_models:\n",
    "#         print(model)\n",
    "#         continue\n",
    "#     x = sorted(task_data.keys())\n",
    "#     y = [task_data[task] for task in x]\n",
    "#     ax.plot(x, y, label=labels[model], color=colors[idx] if model != 0 else 'black', linewidth=2 if model != 0 else 3, markersize=7, linestyle=\"-\")\n",
    "\n",
    "ax.set_xlabel('Model after task', fontsize=24)\n",
    "ax.set_ylabel(METRIC_TO_NAME[METRIC_NAME], fontsize=24)\n",
    "ax.set_xticklabels(list(tasks.keys()), fontsize=20)\n",
    "ax.set_xlim((TASK_IDX, 10))\n",
    "ax.set_ylim(0.15, 0.7)\n",
    "ax.set_yticks([0.2, 0.3, 0.4, 0.5])\n",
    "ax.set_yticklabels([0.2, 0.3, 0.4, 0.5], fontsize=20)\n",
    "# ax.set_yticklabels(fontsize=15)\n",
    "# ax.set_title(f\"Models performance on task #{TASK_IDX}\")\n",
    "ax.legend(fontsize=18, loc=\"upper right\")\n",
    "\n",
    "# Use tight layout to ensure everything fits\n",
    "plt.tight_layout(rect=[0, 0, 1, 1])\n",
    "\n",
    "# Save the figure to file (make sure this comes after the plot commands)\n",
    "plt.savefig(f\"models_object.pdf\")\n",
    "plt.close()\n",
    "# Show the plot (optional)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0],y2[0],y3[0],y4[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_results = Path(\"results/style/base/after_task_10\").resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_base_style = [\n",
    "    load_pickle(path_results / Path(f\"on_task_{task_idx}.pkl\"))[\"samples\"] for task_idx in range(1, 11)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_config = Path(\"data/data_style/config.json\").resolve()\n",
    "with open(path_config) as f:\n",
    "    file = json.load(f)\n",
    "    tasks = file[\"tasks\"]\n",
    "\n",
    "    tasks_map = {int(idx) + 1: task for idx, task in enumerate(tasks)}\n",
    "    for task_k, task_v in tasks_map.items():\n",
    "        task_v[\"index\"] = task_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_style = {}\n",
    "for task_idx in [task_idx for task_idx in tasks_map.keys()]:\n",
    "    on_task_samples = imgs_base_style[task_idx - 1]\n",
    "    eval_path = get_task_eval_path(tasks_map[task_idx], \"style\")\n",
    "    results_style[task_idx] = calculate_metrics(\n",
    "        model_samples=on_task_samples, eval_path=eval_path, task_type=\"style\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csd = [metrics_results[\"csd\"] for metrics_results in results_style.values()]\n",
    "dino = [metrics_results[\"dino\"] for metrics_results in results_style.values()]\n",
    "np.mean(csd, axis=0), np.mean(dino, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_res_from_path(path):\n",
    "    path = Path(path).resolve()\n",
    "    with open(path) as f:\n",
    "        tasks_config = json.load(f)\n",
    "\n",
    "    new_tasks_map = []\n",
    "    for task in tasks_config['tasks']:\n",
    "        for idx, task_base in tasks_map.items():\n",
    "            if task['style'] == task_base['style']:\n",
    "                new_tasks_map.append(idx)\n",
    "                break\n",
    "    return new_tasks_map\n",
    "\n",
    "task_map_1 = load_res_from_path(\"models/merge_and_init/seed_0_style/seed_0_order/config.json\")\n",
    "task_map_2 = load_res_from_path(\"models/merge_and_init/seed_0_style/seed_5_order/config.json\")\n",
    "task_map_3 = load_res_from_path(\"models/merge_and_init/seed_0_style/seed_10_order/config.json\")\n",
    "task_map_4 = load_res_from_path(\"models/merge_and_init/seed_0_style/seed_42_order/config.json\")\n",
    "\n",
    "results1 = {\n",
    "    task_idx: results_style[task_map_1[task_idx]] for task_idx in range(len(task_map_1))\n",
    "}\n",
    "results2 = {\n",
    "    task_idx: results_style[task_map_2[task_idx]] for task_idx in range(len(task_map_2))\n",
    "}\n",
    "results3 = {\n",
    "    task_idx: results_style[task_map_3[task_idx]] for task_idx in range(len(task_map_3))\n",
    "}\n",
    "results4 = {\n",
    "    task_idx: results_style[task_map_4[task_idx]] for task_idx in range(len(task_map_4))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PATH_METRICS_ROOT = Path(\"./results/metrics/\")\n",
    "STYLE_METRICS = {\n",
    "    \"csd\": csd_metric,\n",
    "    \"dino\": dino_metric,\n",
    "}\n",
    "\n",
    "results_final = {\n",
    "    metric_name: {\n",
    "        task_idx + 1: np.mean([results1[task_idx][metric_name], results2[task_idx][metric_name], results3[task_idx][metric_name], results4[task_idx][metric_name]])\n",
    "        for task_idx in range(10)\n",
    "    }\n",
    "    for metric_name in STYLE_METRICS.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "METRIC_NAME = \"dino\"\n",
    "METHOD_NAME = \"mag_max_light\"\n",
    "METHOD_NAME2 = \"naive_cl\"\n",
    "METHOD_NAME3 = \"ortho_init\"\n",
    "METHOD_NAME4 = \"merge_and_init\"\n",
    "TASK_IDX = 2\n",
    "\n",
    "METHOD_TO_NAME = {\n",
    "    \"mag_max_light\": \"Magnitude-based Merging\",\n",
    "    \"naive_cl\": \"Na誰ve Continual Fine-Tuning\",\n",
    "    \"ortho_init\": \"Merge & Orthogonal Initialization\",\n",
    "    \"merge_and_init\": \"Merge & Initialization\",\n",
    "}\n",
    "\n",
    "METRIC_TO_NAME = {\n",
    "    \"clip\": \"CLIP-I\",\n",
    "    \"dino\": \"DINO\",\n",
    "    \"csd\": \"CSD\",\n",
    "}\n",
    "\n",
    "path = Path(f\"results/metrics/style/{METHOD_NAME}/metrics_avg.pkl\").resolve()\n",
    "path2 = Path(f\"results/metrics/style/{METHOD_NAME2}/metrics_avg.pkl\").resolve()\n",
    "path3 = Path(f\"results/metrics/style/{METHOD_NAME3}/metrics_avg.pkl\").resolve()\n",
    "path4 = Path(f\"results/metrics/style/{METHOD_NAME4}/metrics_avg.pkl\").resolve()\n",
    "\n",
    "with open(path, \"rb\") as f:\n",
    "    metrics = pickle.load(f)[METRIC_NAME]\n",
    "\n",
    "with open(path2, \"rb\") as f:\n",
    "    metrics2 = pickle.load(f)[METRIC_NAME]\n",
    "\n",
    "with open(path3, \"rb\") as f:\n",
    "    metrics3 = pickle.load(f)[METRIC_NAME]\n",
    "\n",
    "with open(path4, \"rb\") as f:\n",
    "    metrics4 = pickle.load(f)[METRIC_NAME]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "exclude_models = []\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"axes.facecolor\": \"#f0f0f0\",\n",
    "        \"figure.facecolor\": \"#ffffff\",\n",
    "        \"grid.color\": \"white\",\n",
    "        \"axes.edgecolor\": \"#393939\",\n",
    "        \"axes.linewidth\": 1.0,\n",
    "        \"axes.labelcolor\": \"#393939\",\n",
    "        \"xtick.color\": \"#393939\",\n",
    "        \"ytick.color\": \"#393939\",\n",
    "        \"font.size\": 12,\n",
    "        \"legend.fontsize\": 10,\n",
    "        \"legend.title_fontsize\": 10,\n",
    "        \"figure.figsize\": (14, 7),\n",
    "    }\n",
    ")\n",
    "\n",
    "tasks = defaultdict()\n",
    "tasks2 = defaultdict()\n",
    "tasks3 = defaultdict()\n",
    "tasks4 = defaultdict()\n",
    "\n",
    "data = metrics\n",
    "data2 = metrics2\n",
    "data3 = metrics3\n",
    "data4 = metrics4\n",
    "data3 = metrics3\n",
    "\n",
    "\n",
    "labels = [\"\" for _ in range(len(list(data.keys())) + 1)]\n",
    "data[(1, 2)] = data[(1, 1)]\n",
    "data2[(1, 2)] = data2[(1, 1)]\n",
    "data3[(1, 2)] = data3[(1, 1)]\n",
    "data4[(1, 2)] = data4[(1, 1)]\n",
    "for (model_idx, task_idx) in data.keys():\n",
    "    if task_idx != TASK_IDX:\n",
    "        continue\n",
    "    labels[model_idx] = f\"#{model_idx} tasks\"\n",
    "    tasks[model_idx] = data[(model_idx, TASK_IDX)]\n",
    "\n",
    "for (model_idx, task_idx) in data2.keys():\n",
    "    if task_idx != TASK_IDX:\n",
    "        continue\n",
    "    tasks2[model_idx] = data2[(model_idx, TASK_IDX)]\n",
    "\n",
    "for (model_idx, task_idx) in data3.keys():\n",
    "    if task_idx != TASK_IDX:\n",
    "        continue\n",
    "    tasks3[model_idx] = data3[(model_idx, TASK_IDX)]\n",
    "\n",
    "for (model_idx, task_idx) in data4.keys():\n",
    "    if task_idx != TASK_IDX:\n",
    "        continue\n",
    "    tasks4[model_idx] = data4[(model_idx, TASK_IDX)]\n",
    "\n",
    "\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, 11))\n",
    "tasks[0] = {}\n",
    "labels[0] = \"Base model\"\n",
    "tasks[0] = results_final[METRIC_NAME][TASK_IDX]\n",
    "\n",
    "# Start a new figure (ensure it's created before plotting)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot each model's performance over the tasks\n",
    "x = sorted([model_idx for model_idx in tasks.keys() if model_idx not in [0] + exclude_models])\n",
    "y = [tasks[model_idx] for model_idx in x]\n",
    "ax.plot(x, y, label=METHOD_TO_NAME[METHOD_NAME], color=\"rebeccapurple\", linewidth=3, markersize=7, linestyle=\"-\")\n",
    "\n",
    "y2 = [tasks2[model_idx] for model_idx in x]\n",
    "y2[0] = y[0]\n",
    "ax.plot(x, y2, label=METHOD_TO_NAME[METHOD_NAME2], color=\"darkcyan\", linewidth=3, markersize=7, linestyle=\"-\")\n",
    "\n",
    "y3 = [tasks3[model_idx] for model_idx in x]\n",
    "y3[0] = y[0]\n",
    "ax.plot(x, y3, label=METHOD_TO_NAME[METHOD_NAME3], color=\"goldenrod\", linewidth=3, markersize=7, linestyle=\"-\")\n",
    "\n",
    "y4 = [tasks4[model_idx] for model_idx in x]\n",
    "y4[0] = y[0]\n",
    "ax.plot(x, y4, label=METHOD_TO_NAME[METHOD_NAME4], color=\"indianred\", linewidth=3, markersize=7, linestyle=\"-\")\n",
    "\n",
    "\n",
    "plt.axhline(y=tasks[0], color='black', linestyle='--', linewidth=3, label=\"Base model performance\")\n",
    "\n",
    "ax.set_xlabel('Model after task', fontsize=24)\n",
    "ax.set_ylabel(METRIC_TO_NAME[METRIC_NAME], fontsize=24)\n",
    "ax.set_xticklabels(sorted(list(tasks.keys()))[1:], fontsize=20)\n",
    "ax.set_xlim((1, 10))\n",
    "ax.set_ylim(0.1, 0.5)\n",
    "ax.set_yticks([0.2, 0.3, 0.4])\n",
    "ax.set_yticklabels([0.2, 0.3, 0.4], fontsize=20)\n",
    "# ax.set_yticklabels(fontsize=15)\n",
    "# ax.set_title(f\"Models performance on task #{TASK_IDX}\")\n",
    "ax.legend(fontsize=18, loc=\"upper right\")\n",
    "\n",
    "# Use tight layout to ensure everything fits\n",
    "plt.tight_layout(rect=[0, 0, 1, 1])\n",
    "\n",
    "# Save the figure to file (make sure this comes after the plot commands)\n",
    "plt.savefig(f\"models_style.pdf\")\n",
    "plt.close()\n",
    "# Show the plot (optional)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0],y2[0],y3[0],y4[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Styles task vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_diagonal(matrix, idx):\n",
    "    return np.diag(matrix, k=-idx+1)\n",
    "\n",
    "def extract_means_stds(method_name):\n",
    "    task_vectors_path = Path(\"results/results-conflicts/style\").resolve()\n",
    "    task_vectors = [\n",
    "        pd.read_csv(task_vectors_path / Path(f\"{method_name}/order_{o_idx}/seed_{s_idx}/sign_conflicts_avg_norm.csv\"), index_col=0, header=0) for o_idx in [0, 5] for s_idx in [0,5]\n",
    "    ]\n",
    "    arrays = [\n",
    "        task_vectors[idx].to_numpy()[1:,:-1] for idx in range(len(task_vectors))\n",
    "    ]\n",
    "    for array in arrays:\n",
    "        array[:-1, 0] = np.nan\n",
    "    stacked_arrays = np.stack(arrays, axis=0)\n",
    "\n",
    "    diags_means = []\n",
    "    diags_stds = []\n",
    "    for diag_idx in range(1, 10):\n",
    "        diagonals = np.array([extract_diagonal(matrix, diag_idx) for matrix in stacked_arrays])\n",
    "        mean_diag = np.nanmean(diagonals)\n",
    "        std_diag = np.nanstd(diagonals)\n",
    "        diags_means.append(mean_diag)\n",
    "        diags_stds.append(std_diag)\n",
    "\n",
    "    diags_means = np.array(diags_means)\n",
    "    diags_stds = np.array(diags_stds)\n",
    "\n",
    "    return diags_means, diags_stds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"axes.facecolor\": \"#f0f0f0\",\n",
    "        \"figure.facecolor\": \"#ffffff\",\n",
    "        \"grid.color\": \"white\",\n",
    "        \"axes.edgecolor\": \"#393939\",\n",
    "        \"axes.linewidth\": 1.0,\n",
    "        \"axes.labelcolor\": \"#393939\",\n",
    "        \"xtick.color\": \"#393939\",\n",
    "        \"ytick.color\": \"#393939\",\n",
    "        \"font.size\": 12,\n",
    "        \"legend.fontsize\": 10,\n",
    "        \"legend.title_fontsize\": 10,\n",
    "        \"figure.figsize\": (14, 7),\n",
    "    }\n",
    ")\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "METHOD_TO_NAME = {\n",
    "    \"mag_max_light\": \"Magnitude-based Merging\",\n",
    "    \"naive_cl\": \"Na誰ve Continual Fine-Tuning\",\n",
    "    \"ortho_init\": \"Merge & Orthogonal Initialization\",\n",
    "    \"merge_and_init\": \"Merge & Initialization\",\n",
    "}\n",
    "\n",
    "mml_diag_means, mml_diag_stds = extract_means_stds(\"mag_max_light\")\n",
    "ncl_diag_means, ncl_diag_stds = extract_means_stds(\"naive_cl\")\n",
    "ortai_diag_means, ortai_diag_stds = extract_means_stds(\"ortho_init\")\n",
    "mai_diag_means, mai_diag_stds = extract_means_stds(\"merge_and_init\")\n",
    "\n",
    "x = np.arange(1, 10)\n",
    "ax.plot(x, mml_diag_means, label=\"Magnitude-based Merging\", color='rebeccapurple', linewidth=3)\n",
    "ax.fill_between(x, mml_diag_means - mml_diag_stds, mml_diag_means + mml_diag_stds, color='rebeccapurple', alpha=0.3)\n",
    "\n",
    "ax.plot(x, ncl_diag_means, label=\"Na誰ve Continual Fine-Tuning\", color='darkcyan', linewidth=3)\n",
    "ax.fill_between(x, ncl_diag_means - ncl_diag_stds, ncl_diag_means + ncl_diag_stds, color='darkcyan', alpha=0.3)\n",
    "\n",
    "ax.plot(x, ortai_diag_means, label=\"Merge & Orthogonal Initialization\", color='goldenrod', linewidth=3)\n",
    "ax.fill_between(x, ortai_diag_means - ortai_diag_stds, ortai_diag_means + ortai_diag_stds, color='goldenrod', alpha=0.3)\n",
    "\n",
    "ax.plot(x, mai_diag_means, label=\"Merge & Initialization\", color='indianred', linewidth=3)\n",
    "ax.fill_between(x, mai_diag_means - mai_diag_stds, mai_diag_means + mai_diag_stds, color='indianred', alpha=0.3)\n",
    "\n",
    "ax.set_xlabel('# tasks introduced', fontsize=24)\n",
    "ax.set_ylabel('Opposed params vector to \\ntask adapter ratio', fontsize=24)\n",
    "ax.set_xticklabels(x, fontsize=20)\n",
    "ax.set_xlim((1, 9))\n",
    "ax.set_ylim(0.45, 2.0)\n",
    "ax.set_yticks([0.6, 1.0, 1.4, 1.8])\n",
    "ax.set_yticklabels([0.6, 1.0, 1.4, 1.8], fontsize=20)\n",
    "# ax.set_yticklabels(fontsize=15)\n",
    "# ax.set_title(f\"Models performance on task #{TASK_IDX}\")\n",
    "ax.legend(fontsize=17, loc=\"lower right\")\n",
    "\n",
    "# Use tight layout to ensure everything fits\n",
    "plt.tight_layout(rect=[0, 0, 1, 1])\n",
    "\n",
    "# # Save the figure to file (make sure this comes after the plot commands)\n",
    "plt.savefig(f\"ratio_style.pdf\")\n",
    "plt.close()\n",
    "# Show the plot (optional)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
